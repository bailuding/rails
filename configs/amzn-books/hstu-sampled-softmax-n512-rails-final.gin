# Run this as:
# mkdir -p logs/amzn-books-l50/
# CUDA_VISIBLE_DEVICES=0,1 python3 train.py --gin_config_file=configs/amzn-books/hstu-sampled-softmax-n512-rails-final.gin --master_port=12345 2>&1 | tee logs/amzn-books-l50/hstu-sampled-softmax-n512-rails-final.log
#                                 | full/hr@1      full/hr@10     full/hr@50     full/hr@200    full/ndcg@10   full/ndcg@50   full/ndcg@200  full/mrr      
#   HSTU (b=16, h=4, d=64, n=512) | 0.0102         0.0462         0.1042         0.1845         0.0256         0.0381         0.0502         0.0232        
#   HSTU+MoL run 1                | 0.0157 (53.9%) 0.0632 (37.1%) 0.1303 (25.0%) 0.2184 (18.4%) 0.0363 (42.4%) 0.0509 (33.6%) 0.0641 (27.9%) 0.0326 (41.1%)
#   HSTU+MoL run 2                | 0.0150 (47.1%) 0.0613 (33.0%) 0.1292 (24.0%) 0.2167 (17.5%) 0.0350 (37.3%) 0.0498 (30.7%) 0.0629 (25.5%) 0.0315 (36.4%)

train_fn.dataset_name = "amzn-books"
train_fn.max_sequence_length = 50
train_fn.local_batch_size = 64
train_fn.eval_batch_size = 64
train_fn.eval_user_max_batch_size = 64  # otherwise need to fix uid logic
train_fn.item_feature_embedding_dim = 0

train_fn.main_module = "HSTU"
train_fn.main_module_bf16 = True
train_fn.item_embedding_dim = 64
train_fn.dropout_rate = 0.5
train_fn.user_embedding_norm = "l2_norm"
train_fn.item_embedding_dim = 64

hstu_encoder.num_blocks = 16
hstu_encoder.num_heads = 8
hstu_encoder.dqk = 8
hstu_encoder.dv = 8
hstu_encoder.linear_dropout_rate = 0.5

train_fn.eval_interval = 1000
train_fn.num_epochs = 201
train_fn.learning_rate = 1e-3
train_fn.weight_decay = 0
train_fn.num_warmup_steps = 0

# train_fn.save_ckpt_every_n = 5

train_fn.interaction_module_type = "DotProduct"
train_fn.top_k_method = "MIPSBruteForceTopK"

train_fn.loss_module = "SampledSoftmaxLoss"
train_fn.num_negatives = 512
train_fn.temperature = 0.05

train_fn.sampling_strategy = "local"
train_fn.item_l2_norm = True
train_fn.l2_norm_eps = 1e-6

train_fn.enable_tf32 = True
train_fn.full_eval_every_n = 5
train_fn.partial_eval_num_iters = 32

create_data_loader.prefetch_factor = 1024
create_data_loader.num_workers = 8
